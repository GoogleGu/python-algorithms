# 机器学习算法：线性回归

线性回归是最简单的一类机器学习算法，其本质是监督学习。下面举一个现实场景中的例子，用线性回归来解决这个问题。
<!-- more -->
## 1. 问题场景
我们现在收集到了一个班级里学生的智商、每日学习时间以及数学成绩的数据。假设一共有50个学生，也即我们有50组数据，现在想根据这组数据来预测其他已知智商、每日学习时间的学生的数学成绩。
在这里我们需要引入一系列的符号来表示各个量。

### 1.1.符号表示
$n$: 特征数量，在这个例子里我们有两个特征，智商和每日学习时间。
$m$: 训练数据组数，在这个例子里我们有50组数据。
$x$: 输入变量，也即特征的输入值。
$y$: 目标变量，也即输出的值。 
$(x, y)$: 一组训练数据。
$x^{(i)}, y^{(i)}$: 第i组训练数据。
$h$: hypothesis假说，也即预测函数，其输入为一组输入变量，然后返回预测的输出值。

### 1.2. 预测函数
在这里，由于是使用线性回归，我们大胆预测智商、每日学习时间、数学成绩这三个变量是线性相关的，也即：
$h(x)=\theta_0+\theta_1x_1+\theta_2x_2$
其中$x_1$为智商输入，$x_2$为每日学习时间，$h(x)$为预测的成绩。
为了便于矩阵运算，我们用$x_0$表示$\theta_0$对应的输入变量，且令$x_0=1$，则对于n个特征数量的问题，该式子可以写作：
$h(x)=\sum_{i=0}^{n} \theta_ix_i$

**矩阵表达**
如果用向量$X=[x_0 ... x_n]^T, X\in R^{n+1}$，向量$\theta=[\theta_0 ... \theta_n]^T, \theta\in R^{n+1}$来表示的话，可以将其改写为：
$h_{\theta}(X) = \theta^TX$

## 1.3. 代价函数cost function
现在我们已经可以通过预测函数，对一组输入$x_1, x_2$智商与学习时间获得预测的成绩$y$，但是预测的准确度如何，是用代价函数$J$来衡量的，其定义是对训练数据对应的预测值与实际输出值的平方求和，也即最小平方和，如下：
$J(\theta) = \frac{1}{2} \sum_{i=1}^{m} (\theta^TX^{(i)} - Y^{(i)})^2$，这里对平方和多乘以了一个$\frac{1}{2}$是为了计算上的方便。
我们的目的就是找到合适的参数$\theta$来让$J$的值达到最小。

## 2. 用梯度下降的方法来求解
假设我们选定一组随机的初始参数$\theta^j$值，我们希望可以找到让代价函数更小的一组$\theta^{j+1}$值，这时候我们只要对代价函数求导，并且用原来的$\theta^{j+1}$减去代价函数在$\theta^j$处的导数即可：
$\theta^{j+1} = \theta^j - a \cdot \frac{1}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y{(i)}) \cdot x_j^{(i)} $
其中$a$被称为学习系数，也即在每一次的迭代中，我们要向代价函数更小的方向相对前进多少。

**矩阵表达**
如果用$\bigtriangledown_\theta J$来表示对$J(\theta)$的导数，也即$\bigtriangledown_\theta J = [\frac{\partial J}{\partial \theta_0} ... \frac{\partial J}{\partial \theta_n}]^T \in R^{n+1}$
就可以把上面的式子用矩阵乘法表示：$\theta^{j+1} = \theta^j - a \cdot \bigtriangledown_\theta J$
重复该步骤，直到$J(\theta_j+1)和J(\theta_j)$的差值变得很小，此时我们可以认为已经收敛，我们找到了最佳的$\theta$的最优解。
关于梯度下降，是一个值得单独拎出来讲的部分，详情见[# 机器学习笔记（二）：梯度下降](mweblib://15471290823707)

## 3. 用正规方程求解
除了梯度下降外，我们其实可以直接算出最佳的$\theta$值，因为在$J_\theta)$的极值必然出现在其导数为0的地方，因此我们直接计算$\bigtriangledown_\theta J = 0$即可，用矩阵计算化简后为：
$\theta = (X^T X)^{-1}X^TY), X \in R^{m\times(n+1)}, Y \in R^m$(推导过程略)
这个式子计算的瓶颈在于对矩阵求逆$(X^T X)^{-1}$，这是一个n+1乘以n+1的矩阵，对于矩阵维度来说，对矩阵求逆是$O(n^2)$的时间复杂度。因此如果n特别大也即我们模型特征数量特别大时，用正规方程求解会很慢，最好用梯度下降求解；在特征数量不是特别大的时候，正规方程是不错的选择。
